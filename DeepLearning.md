## lesson 1（理论）

<img src="C:\Users\xn666\AppData\Roaming\Typora\typora-user-images\image-20221028223116360.png" alt="image-20221028223116360" style="zoom:33%;" />

右上角标，小括号——样本数，中括号——神经网络层数，大括号——Mini Batch个数

###### logistics

​	一种包含log对数的cost函数，衡量模型好坏(核心思想是***<u>最大似然求概率</u>***)

​	lost = 预测输出减实际
​	cost = 训练集lost平均值

​	一般使用<u>ReLu</u>激活函数

------

###### Python

​	向量化（矩阵）并行运算 替代for循环 加快速度  （利用广播特点：沿所需方向周期延拓）
*<u>	reshape</u>*（修正秩是1的矩阵）

###### 神经网络

​	backpropogation 就是链式求导，目的 : 求梯度下降法中的各参数梯度，cost对线性输出，非线性输出（激活输出），线性参数的导数。最终达到 使用 ***<u>梯度下降法</u>*** 更新 **<u>参数</u>** 的目的

​	求导的话，我们需要缓存前向传输的**<u>线性输出值</u>**

<img src="C:\Users\xn666\AppData\Roaming\Typora\typora-user-images\image-20221028221951607.png" alt="image-20221028221951607" style="zoom: 33%;" />

###### 超参数

​	学习率，迭代次数，隐含层数量和节点数量，激活函数。      这些都会影响***<u>最终参数</u>***

## lesson2（实操）

###### 目的

​	训练集准确率 ——bias         more deeper or bigger network	

​	测试集准确率 ——variance		more data , *<u>Regularization</u>*

###### 正则化

###### L2正则化

​	cost函数+参数的正则项（范数平方）    ——————导致backpropagation求导，参数的梯度下降的更快

​	***<u>直观理解：</u>***使参数接近于0，减少隐藏层的影响，神经网络变得更简单，防止过拟合

###### Dropout正则化

​	遍历节点，随机删除，概率为drop-probe。注意输出需要再除个drop-probe，以保持输出期望。

​	***<u>原理：</u>***去除权重大的节点，收缩各层权重的平方范数。减小复杂度较高的隐含层（<u>越复杂，drop-probe越小</u>）的复杂度，防止过拟合。

​	缺点：不利于交叉验证（需要更多超参数）。难以计算cost函数。一般用于CV领域。

## lesson3（优化梯度下降）

###### Mini Batch

​	原理：样本数量太多时，将样本集拆分成一个个小样本集，其他计算不变。

​	结果：由于样本集不同，每次迭代后梯度下降会产生噪声，但总体趋势下降。

​	特例：当样本集大小为1时——***<u>随机梯度</u>***下降（噪声更大，不能利用并行处理，效率低）。当样本集大小就是原始样本集时——batch梯度下降（每次迭代时间长）

##### 优化算法（引入超参数）

###### 指数加权（移动）平均

​	特点：均值加权不再是1/n，而变成了指数形式的等比数列形式

​	优点：计算样本均值不需要保留所有样本值，只需保留前一时刻样本均值和本时刻样本值即可，节省内存

​    缺点：前期存在很大偏差，需要偏差修正，当然你也可以熬过前期

###### 动量梯度下降法

​	原理：用梯度的***指数加权平均梯度***替代梯度，降低上下摆动幅度（0.9）

​	目的：正常的梯度下降法学习时存在上下摆动的现象，减缓了学习速度。

<img src="C:\Users\xn666\AppData\Roaming\Typora\typora-user-images\image-20221101203526733.png" alt="image-20221101203526733" style="zoom: 25%;" />



###### RMSprop

​	求动量***平方***梯度，改造学习公式中的微分，微分除以动量***平方***梯度（0.99,10^（-8））

​	本质：都是改变梯度下降公式，影响超参数调整的方向，使其朝着主要方向前进。

###### Adam

​	结合前两者

<img src="C:\Users\xn666\AppData\Roaming\Typora\typora-user-images\image-20221101205951756.png" alt="image-20221101205951756" style="zoom: 33%;" />

###### 学习率下降

###### 局部最优

​	由于一般cost函数维度很大，且存在很多参数，所以不太可能会出现滞留在局部最优区域。

​	但是在局部最优区域学习速度将会很慢。

​	动量梯度，RMSprop，Adam就是为了加快在这些区域中的学习速度。

## lesson4（超参数设定）

###### 超参数设置非线性范围(对数)

###### Batch归一化

​	对激活函数输入进行归一化

###### softmax

​	使输出物理意义变为概率，用e指数处理原输出。(输出归一化)
​	即做一个输出数值大小到概率的映射(激活函数）。
​	实质：多元分类。当输出为个数为2时，相当于logistic回归。

###### 神经网络思路

​	寻找样本对应预测输出的loss，使之可以代表预测效果，所有样本loss均值后得到cost。利用backpropagation梯度下降调整参数，不断使cost最小化。

## lesson5（学习策略）

###### 可避免偏差	

​	可避免偏差又称贝叶斯错误率：人错误率和训练集错误率之差。通过人错误率可以估计贝叶斯错误率。

​	方差：训练集错误率和测试集错误率之差

​	启发：我们要选择关注偏差（扩大神经网络）还是关注方差（正则化，扩大数据集），如果偏差很大优先关注偏差

## lesson4（超参数设定）



















